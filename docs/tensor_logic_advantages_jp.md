# Tensor Logic: なぜ単なるテンソルよりも多くの問題を解けるのか

## 概要

**Tensor Logic** は、テンソル演算と論理的推論を統合したプログラミングパラダイムです。従来のテンソルフレームワーク（PyTorch、TensorFlowなど）は主に数値計算と微分可能な関数に焦点を当てていますが、Tensor Logicはこれに**論理的構造**、**制約充足**、**記号的推論**を追加することで、解決可能な問題の範囲を大幅に拡張します。

---

## 1. 従来のテンソル演算の限界

### 1.1 数値的アプローチの問題点

従来のテンソル演算は以下のような問題に直面します：

```
# 典型的なディープラーニングパイプライン
x = tensor([...])
y = model(x)  # 順伝播
loss = criterion(y, target)
loss.backward()  # 逆伝播
```

**限界**:
- **離散的最適化**: 整数解や組み合わせ問題（TSP、グラフ着色など）に弱い
- **制約の表現**: 「少なくとも1つ」「正確に1つ」などの論理制約を自然に表現できない
- **記号的推論**: 「AならばB」のような論理的関係を直接エンコードできない
- **逆問題**: 結果から原因を推論する問題が困難

### 1.2 具体例：N-Queens問題

純粋なテンソルアプローチでN-Queens問題を解こうとすると：

```python
# PyTorch的アプローチ
board = torch.softmax(logits, dim=-1)  # 各行で確率分布
loss = -torch.log(board.max())  # ??? どの制約を最適化？
```

この方法では、「各行に正確に1つのクイーン」「対角線上に衝突なし」といった制約を自然に表現・最適化することが困難です。

---

## 2. Tensor Logicの拡張

Tensor Logicは以下の概念を導入することで、問題解決能力を拡張します：

### 2.1 論理演算のテンソル化

**命題をテンソルとして表現**:
```
// 各要素が「その命題が真かどうか」を表す
let queen = Tensor::zeros([N, N]); // queen[i,j] = 1.0 if queen at (i,j)
```

**論理演算子の微分可能な近似**:
| 論理演算 | 数式 | テンソル式 |
|----------|------|-----------|
| NOT(A) | 1 - A | `1.0 - A` |
| AND(A,B) | A × B | `A * B` |
| OR(A,B) | A + B - A×B | `A + B - A * B` |
| A → B | 1 - A + A×B | `1.0 - A + A * B` |

これにより、論理式を微分可能な損失関数に変換できます。

### 2.2 制約の損失関数への変換

**「各行に正確に1つ」の制約**:
```
// 行の合計が1であるべき
let row_sum = queen.sum(1);           // [N] 各行の合計
let row_loss = (row_sum - 1.0).pow(2).sum();
```

**「対角線上に衝突なし」の制約**:
```
// 対角線上のクイーンの積が0であるべき
let diag_loss = compute_diagonal_conflicts(queen);
```

**統合損失**:
```
let total_loss = row_loss + col_loss + diag_loss;
total_loss.backward();
```

### 2.3 記号的構造のエンコーディング

Tensor Logicでは、グラフ構造や関係を自然にエンコードできます：

**グラフの隣接行列**:
```
let adj = Tensor::zeros([N, N]);  // adj[i,j] = 1 if edge i→j
```

**関係の推論（推移閉包）**:
```
// "友達の友達も友達" のような推論
let friends_of_friends = adj.matmul(adj);
let transitive = (adj + friends_of_friends).clamp(0.0, 1.0);
```

---

## 3. Tensor Logicで解ける新しい問題クラス

### 3.1 組み合わせ最適化問題

| 問題 | 説明 | Tensor Logicアプローチ |
|------|------|------------------------|
| **N-Queens** | N×Nボードにクイーンを配置 | 衝突制約を損失関数化 |
| **TSP** | 最短巡回路を見つける | 順列行列をSoftmaxで近似 |
| **グラフ着色** | 隣接頂点に異なる色を割当て | 色行列と隣接制約 |
| **SAT** | 論理式の充足可能性 | 節を積で、リテラルを変数で表現 |

### 3.2 逆問題

| 問題 | 説明 | Tensor Logicアプローチ |
|------|------|------------------------|
| **Inverse Life** | 結果状態から初期状態を推論 | 順伝播を微分可能にして逆最適化 |
| **逆レンダリング** | 画像からシーンを復元 | ソフトレンダリングで勾配を伝播 |
| **回路学習** | 真理値表から回路を学習 | ゲートをsigmoidで近似 |

### 3.3 構造化推論

| 問題 | 説明 | Tensor Logicアプローチ |
|------|------|------------------------|
| **知識グラフ補完** | 欠損リンクを推論 | 関係をテンソルでエンコード |
| **プログラム合成** | 例から関数を合成 | 離散的操作を連続緩和 |
| **パターンマッチング** | 構造的パターンを検出 | 畳み込みと論理演算の組み合わせ |

---

## 4. 具体例で見るTensor Logicの力

### 4.1 例1: ライフゲームの逆問題

**問題**: 5ステップ後に特定の状態になる初期状態を見つけよ

**従来のアプローチの困難点**:
- ライフゲームは離散的（0/1）
- カオス的挙動があり、解析的な逆関数は存在しない
- 組み合わせ爆発（2^(N×N) の可能な初期状態）

**Tensor Logicによる解決**:
```
// 1. ルールを微分可能に近似
let neighbors = state.conv2d(kernel, 1, 1) - state;
let is_3 = ((neighbors - 3.0).pow(2) * -1.0).exp();  // 隣接3で誕生
let is_2 = ((neighbors - 2.0).pow(2) * -1.0).exp();  // 隣接2で生存
let next_state = is_3 + state * is_2;

// 2. 逆問題を最適化
let learnable_init = Tensor::randn([N, N], true);
let evolved = forward(learnable_init.sigmoid(), STEPS);
let loss = (evolved - target).pow(2).sum();
loss.backward();  // 初期状態への勾配！
```

### 4.2 例2: XOR関数の学習

**問題**: XOR真理値表を満たす論理回路を学習せよ

**なぜ興味深いか**:
- XORは線形分離不可能（単層パーセプトロンでは解けない）
- デジタルロジックは本質的に離散的

**Tensor Logicによる解決**:
```
// 論理ゲートをsigmoidで近似
let hidden = (inputs.matmul(W1) + b1).sigmoid();
let output = (hidden.matmul(W2) + b2).sigmoid();

// 真理値表との誤差を最小化
let loss = (output - targets).pow(2).sum();
```

### 4.3 例3: 微分可能レイキャスティング

**問題**: 1D投影画像から2Dシーンを復元せよ

**従来の困難点**:
- レイキャスティングは離散的な交差判定
- インデックスアクセスは微分不可能

**Tensor Logicによる解決**:
```
// ハードインデックスの代わりにソフトサンプリング
let weights = ((grid - sample_pos).pow(2) * -sigma).exp();
let sample = (weights * scene).sum() / (weights.sum() + epsilon);
// → 勾配がシーン全体に伝播！
```

---

## 5. 理論的基盤

### 5.1 連続緩和（Continuous Relaxation）

離散変数 x ∈ {0, 1} を連続変数 x̃ ∈ [0, 1] に緩和することで：
- 勾配法が適用可能になる
- 離散解への丸めは後処理

### 5.2 構造的損失関数

制約を満たす解を見つけるために：
```
Loss = Σ(制約違反のペナルティ) + λ × 目的関数
```

### 5.3 微分可能シミュレーション

物理シミュレーションや動的システムを微分可能にすることで：
- 初期条件の最適化
- パラメータ推定
- 制御問題の解決

---

## 6. Tensor Logicの利点まとめ

| 側面 | 従来のテンソル | Tensor Logic |
|------|---------------|--------------|
| **問題表現** | 純粋に数値的 | 論理+数値 |
| **制約** | 正則化項として追加 | ネイティブサポート |
| **離散問題** | 近似困難 | 連続緩和で解決 |
| **逆問題** | 特殊なアーキテクチャ | 順伝播の微分で自然に |
| **記号的推論** | サポートなし | テンソル演算に変換 |
| **説明可能性** | ブラックボックス | 論理構造が可読 |

---

## 7. 結論

Tensor Logicは、**微分可能プログラミング**の力と**記号的推論**の表現力を組み合わせることで、従来のテンソルフレームワークでは困難だった問題クラスを解決可能にします：

1. **組み合わせ最適化**: 離散制約を連続損失に変換
2. **逆問題**: 順方向シミュレーションを微分可能にして逆最適化
3. **論理推論**: 命題論理をテンソル演算にエンコード
4. **構造学習**: 関係やパターンをテンソルで表現

これにより、機械学習、最適化、推論の境界を超えた新しいアルゴリズムの設計が可能になります。

---

## 参考リンク

- [N-Queens例](../examples/tasks/tensor_logic/n_queens/README_JP.md)
- [TSPソルバー例](../examples/tasks/tensor_logic/tsp/README_JP.md)
- [逆ライフゲーム例](../examples/tasks/tensor_logic/inverse_life/README_JP.md)
- [デジタルロジック例](../examples/tasks/tensor_logic/digital_logic/README_JP.md)
- [レイキャスター例](../examples/tasks/tensor_logic/raycast/README_JP.md)
